`    # Ch 1. 導論

    ## 研究動機

    由於離散表徵在當今語音模型與語音處理技術已經愈來愈具備重要性，因此探討與分析為什麼語音離散表徵可以幫助下游任務的背後成因是相當重要的研究方向，其中一個驗證離散表徵能夠幫助模型處理語音訊號的方式，便是驗證其與音位（phoneme）之間的對應關係。 

    ## 研究主題

    離散表徵在語音 community 中常被當成一種類似文字的存在，另外有一些文獻則是將其當成連續表徵的精簡表示法。因此，我們藉由分析各種離散單元和人類理解語音最直接的處理層次：音位（phoneme）之間的關係，並將兩者進行各種統計上的序列比較，可以作為訓練大型語音基石模型（foundation model）的分詞（tokenization）基礎，選定最適合的表徵最為系統的輸入符記（token）。

    ## 章節安排

    由於本論文是以剖析既有的語音離散表徵為主軸，因此就相關研究方面需要從各角度入手，單獨成一章節。接著我們會從單一的離散單元，以及將單元視為像文字的字符（character）並進行分詞演算法兩種對語音離散單元處理的層次分別成章進行分析，最後將這些表徵嘗試做在語音的任務上，以驗證其具有一定的語音表徵能力，且能保留語音學的特徵。

    # Ch 2. 背景知識

    ## 深層類神經網路

    深層類神經網路是一個取法自生物神經連結的數學模型，其在計算認知神經科學中以連結派（connectionism）為主要代表，後在電腦科學與機器學習中有不同結構的進展。在此之後，基於其彈性與平行化的能力，能在 GPU 上面很有效率的進行運算並達到前所未有的效能，因此現在已經成為人工智慧發展的主流。

    基於深層類神經網路的神經架構有 CNN、RNN、Transformer 等等，由於這些架構在語音與文字處理上都已經被廣泛使用，因此在下面分別介紹：

    ### CNN
    ### RNN
    ### Transformer

    ## 表徵學習
    ## Attention 機制

    ## 語音基石模型與自監督式學習

    # Ch 3. 相關研究

    在 HuBERT 出來之後，

`